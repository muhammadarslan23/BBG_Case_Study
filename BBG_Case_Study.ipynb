{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eaCx8WiXo5M",
        "outputId": "79c7baa7-71f9-4cbd-f896-29d2afc3a605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=fdaad27a90726a0d29c6d0383eccf2591927a53170a98afb3154b1325062c609\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, expr\n",
        "from pyspark.sql.types import FloatType\n",
        "import datetime\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"ECommerceCaseStudy\").getOrCreate()\n",
        "\n",
        "# Load the data files into a PySpark DataFrame\n",
        "sales_df = spark.read.csv(\"sales_and_traffic_data.csv\", header=True)\n",
        "mapping_df = spark.read.csv(\"amazon_shop_mapping.csv\", header=True)\n",
        "campaign_df = spark.read.json(\"campaign_object.json\")\n",
        "\n",
        "# Define a User-Defined Function (UDF) to convert currency\n",
        "def convert_to_euro(amount, currency):\n",
        "    try:\n",
        "        # Convert currency to Euro based on a fixed exchange rate\n",
        "        exchange_rate = 0.85\n",
        "        amount_euro = float(amount) * exchange_rate\n",
        "        return round(amount_euro, 2)\n",
        "    except ValueError:\n",
        "        return 0.0  # Handle non-numeric or missing values\n",
        "\n",
        "# Register the UDF for conversion\n",
        "convert_udf = spark.udf.register(\"convert_to_euro\", convert_to_euro, FloatType())\n",
        "\n",
        "# Join the sales_df with mapping_df to get currency information\n",
        "joined_df = sales_df.join(mapping_df, \"shop_name\", \"left\")\n",
        "\n",
        "# Perform currency conversion using the UDF\n",
        "sales_df_with_euro = joined_df.withColumn(\n",
        "    \"ordered_products_sale_euro\",\n",
        "    convert_udf(col(\"ordered_products_sale\"), col(\"currency\"))\n",
        ")\n",
        "\n",
        "# Calculate the total revenue\n",
        "total_revenue = sales_df_with_euro.selectExpr(\"round(sum(ordered_products_sale_euro), 2) as total_revenue\").collect()[0][\"total_revenue\"]\n",
        "\n",
        "# total revenue per country\n",
        "revenue_per_country = sales_df_with_euro.groupBy(\"country\").sum(\"ordered_products_sale_euro\")\n",
        "revenue_per_country = revenue_per_country.withColumnRenamed(\"sum(ordered_products_sale_euro)\", \"total_revenue_per_country\")\n",
        "\n",
        "\n",
        "# total revenue per shop\n",
        "revenue_per_shop = sales_df_with_euro.groupBy(\"shop_name\").sum(\"ordered_products_sale_euro\")\n",
        "revenue_per_shop = revenue_per_shop.withColumnRenamed(\"sum(ordered_products_sale_euro)\", \"total_revenue_per_shop\")\n",
        "\n",
        "\n",
        "\n",
        "# the report_date column to a date format\n",
        "sales_df_with_euro = sales_df_with_euro.withColumn(\"report_date\", col(\"report_date\").cast(\"date\"))\n",
        "\n",
        "sales_df_with_euro = sales_df_with_euro.withColumn(\"month\", expr(\"month(report_date)\"))\n",
        "\n",
        "# total revenue per month\n",
        "revenue_per_month = sales_df_with_euro.groupBy(\"month\").sum(\"ordered_products_sale_euro\")\n",
        "revenue_per_month = revenue_per_month.withColumnRenamed(\"sum(ordered_products_sale_euro)\", \"total_revenue_per_shop\")\n",
        "\n",
        "# Print Results\n",
        "print(\"Total Revenue in Euros:\", total_revenue)\n",
        "revenue_per_country.show()\n",
        "revenue_per_shop.show()\n",
        "revenue_per_month.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA7AcKmfYgqg",
        "outputId": "8d7c69da-4bad-4047-9dfe-2a8270b04774"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Revenue in Euros: 803962.01\n",
            "+-------+-------------------------+\n",
            "|country|total_revenue_per_country|\n",
            "+-------+-------------------------+\n",
            "|     NL|       20841.619942605495|\n",
            "|     PL|       20841.619942605495|\n",
            "|   NULL|        616387.4300203435|\n",
            "|     DE|       20841.619942605495|\n",
            "|     ES|       20841.619942605495|\n",
            "|     TR|       20841.619942605495|\n",
            "|     FR|       20841.619942605495|\n",
            "|     IT|       20841.619942605495|\n",
            "|     SE|       20841.619942605495|\n",
            "|     UK|       20841.619942605495|\n",
            "+-------+-------------------------+\n",
            "\n",
            "+--------------------+----------------------+\n",
            "|           shop_name|total_revenue_per_shop|\n",
            "+--------------------+----------------------+\n",
            "|         BBG BeerCup|    30144.650010395795|\n",
            "|BBG The Friendly ...|     386635.7399078831|\n",
            "|     Elektronik-Star|     34396.09006881714|\n",
            "|    BBG Spielehelden|    249.25999450683594|\n",
            "| Berlin Brands Group|                 50.75|\n",
            "|            SLIMPURO|     87197.68994675577|\n",
            "|Klarstein Deutsch...|    26476.269998028874|\n",
            "|BBG Scandinavian Hub|    276.74000549316406|\n",
            "|             Feelino|     170856.6294388175|\n",
            "|BBG Gramercy Kitc...|     34183.92006659508|\n",
            "| BBG Zelite-Infinity|     346.6400146484375|\n",
            "|           Casa Chic|    16717.950044631958|\n",
            "|     BBG Superlunary|    233.19999885559082|\n",
            "|    skullcap-helmets|    16196.480008363724|\n",
            "+--------------------+----------------------+\n",
            "\n",
            "+-----+----------------------+\n",
            "|month|total_revenue_per_shop|\n",
            "+-----+----------------------+\n",
            "|    1|    179345.82007725537|\n",
            "|    3|     405163.8495291583|\n",
            "|    2|    219452.33989737928|\n",
            "+-----+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, from_json, split, when\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
        "from pyspark.sql.functions import col, explode\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"CampaignAnalysis\").getOrCreate()\n",
        "\n",
        "# Load the campaign_object.json data\n",
        "campaign_data = spark.read.json(\"campaign_object.json\")\n",
        "\n",
        "# Extract the \"CREATIVE\" JSON string from the \"data\" array\n",
        "campaign_df = campaign_data.select(col(\"data\")[0][2].alias(\"CREATIVE\"))"
      ],
      "metadata": {
        "id": "nYGYMv1UbAkF"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the schema for the \"CREATIVE\" column\n",
        "creative_schema = StructType([\n",
        "    StructField(\"brandName\", StringType(), True),\n",
        "    StructField(\"brandLogoAssetID\", StringType(), True),\n",
        "    StructField(\"headline\", StringType(), True),\n",
        "    StructField(\"asins\", StringType(), True),  # Assuming it's a string\n",
        "    StructField(\"brandLogoUrl\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Parse the \"CREATIVE\" column as JSON using the specified schema\n",
        "campaign_data = campaign_data.withColumn(\"CREATIVE_JSON\", from_json(col(\"data\")[0][2], creative_schema))\n",
        "\n",
        "# Access the keys in the \"CREATIVE_JSON\" struct\n",
        "campaign_data.select(\n",
        "    \"CREATIVE_JSON.brandName\",\n",
        "    \"CREATIVE_JSON.brandLogoAssetID\",\n",
        "    \"CREATIVE_JSON.headline\",\n",
        "    \"CREATIVE_JSON.asins\",  # Access it as a string\n",
        "    \"CREATIVE_JSON.brandLogoUrl\"\n",
        ").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdRK1QESbfvF",
        "outputId": "2fda1ab6-7d18-4031-a3ce-d72066f840ab"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------------------------------------------------------------------+--------------------------+----------------------------------------+------------------------------------------------------------------------------------------------+\n",
            "|brandName|brandLogoAssetID                                                     |headline                  |asins                                   |brandLogoUrl                                                                                    |\n",
            "+---------+---------------------------------------------------------------------+--------------------------+----------------------------------------+------------------------------------------------------------------------------------------------+\n",
            "|Pamara   |amzn1.assetlibrary.asset1.198f5d4d083135d4acff704bf947a982:version_v1|Per il tuo futuro luminoso|[\"B06W5543D6\",\"B074F576VM\",\"B078VLNF5K\"]|https://m.media-amazon.com/images/S/al-eu-726f4d26-7fdb/1b6aeff7-5d01-458e-9e26-5afb917cf8aa.png|\n",
            "+---------+---------------------------------------------------------------------+--------------------------+----------------------------------------+------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "asins_df=campaign_data.select('CREATIVE_JSON.asins')\n",
        "asins_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-74K_JdKboJP",
        "outputId": "4e49193b-dc08-4cea-81fa-baa142343a01"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|               asins|\n",
            "+--------------------+\n",
            "|[\"B06W5543D6\",\"B0...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split, col\n",
        "\n",
        "# Split the \"asins\" column into an array of strings\n",
        "df = asins_df.withColumn(\"asins_array\", split(col(\"asins\"), \",\"))\n",
        "\n",
        "# Create columns for each element in the array\n",
        "for i in range(3):  # Change the range as needed\n",
        "    df = df.withColumn(f\"asins_{i + 1}\", col(\"asins_array\")[i])\n",
        "\n",
        "# Select the desired columns\n",
        "df = df.select(\"asins_1\", \"asins_2\", \"asins_3\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSxOPtvObrer",
        "outputId": "cadc5111-96cd-4546-b308-7762e47407ec"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------------+-------------+\n",
            "|      asins_1|     asins_2|      asins_3|\n",
            "+-------------+------------+-------------+\n",
            "|[\"B06W5543D6\"|\"B074F576VM\"|\"B078VLNF5K\"]|\n",
            "+-------------+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, coalesce, split, lit\n",
        "\n",
        "\n",
        "sales_df = spark.read.csv(\"sales_and_traffic_data.csv\", header=True)\n",
        "\n",
        "# Task 3: Extract distinct ASINs from sales_and_traffic_data.csv\n",
        "distinct_asin_df = sales_df.select(\"child_asin\").distinct()\n",
        "\n",
        "# Task 4: Create the \"active_asin\" column based on distinct ASINs\n",
        "distinct_asin_list = [row.child_asin for row in distinct_asin_df.collect()]\n",
        "\n",
        "campaign_df = df.withColumn(\"active_asin\", coalesce(\n",
        "    col(\"asins_1\"),\n",
        "    when(col(\"asins_2\").isin(distinct_asin_list), col(\"asins_2\")),\n",
        "    when(col(\"asins_3\").isin(distinct_asin_list), col(\"asins_3\")),\n",
        "    lit(None)\n",
        "))\n",
        "\n",
        "# Show the transformed campaign DataFrame\n",
        "campaign_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGrprMwtb9Y1",
        "outputId": "5379b717-eac4-43a4-fb03-1d35d1b72ef7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+------------+-------------+-------------+\n",
            "|      asins_1|     asins_2|      asins_3|  active_asin|\n",
            "+-------------+------------+-------------+-------------+\n",
            "|[\"B06W5543D6\"|\"B074F576VM\"|\"B078VLNF5K\"]|[\"B06W5543D6\"|\n",
            "+-------------+------------+-------------+-------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}